{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46aa07c6-a4fd-4272-8b34-7688658d8eb6",
   "metadata": {},
   "source": [
    "# Transfer Learning from Pre-Trained Models  \n",
    "\n",
    "Instead of starting from randomly initialised weights, transfer learning allows you to start with weights from a pre-trained neural network to learn something new.\n",
    "\n",
    "This notebook walkthrough transfer learning from a [VGG11](https://pytorch.org/vision/main/models/generated/torchvision.models.vgg16.html) model trained on [the imagenet dataset](https://www.image-net.org/). There are a lot of pre-trained models in the [torchvision models library](https://pytorch.org/vision/stable/models.html), feel free to explore other options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7294d2ef-7187-44ef-8936-3ed302e4b336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da7cee8-c7b3-43bc-910d-0ac45cb20504",
   "metadata": {},
   "source": [
    "## **Step 1** - Prepare Dataset\n",
    "\n",
    "We'll use the [Flower Recognition](https://aiplanet.com/challenges/61/data-sprint-25-flower-recognition-61/data) dataset again, but this time we'll use images with higher-resolution.\n",
    "\n",
    "If you haven't done this:\n",
    "<u>Please download the dataset from [here](https://drive.google.com/file/d/1v0or9w-z4x0M0SHNFkp9UVZGsJ1G6e_t/view?usp=sharing) and unzip it to the `class-4` folder.</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07eccab4-d43d-4eb7-871d-58e65a2b2e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we choose 224 because this is the input resolution of the VGG16 model\n",
    "image_resolution = 224 \n",
    "\n",
    "# how many colour channels?\n",
    "# 3 for RGB, 1 for greyscale\n",
    "colour_channels = 3\n",
    "\n",
    "# make sure the path to your image folder is correct\n",
    "folder_path = './flowers_n/train'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae522610-c3be-46de-b290-798671c676c7",
   "metadata": {},
   "source": [
    "#### **1.1** - Pre-process images \n",
    "\n",
    "Pre-process (i.e. crop, resize, normalise) the images in our dataset.\n",
    "\n",
    "Use [PyTorch's Transforming functions](https://pytorch.org/vision/0.17/transforms.html) to define the pre-processes pipeline on our images. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4cf3412-0e6d-4cd6-be83-8ce1d868b784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.v2 import Compose, ToImage, ToDtype, Grayscale, Resize, RandomCrop, Normalize\n",
    "\n",
    "# this transformation function will help us pre-process images during the training (on-the-fly)\n",
    "transformation = Compose([   \n",
    "    # convert an image to tensor\n",
    "    ToImage(),\n",
    "    ToDtype(torch.float32, scale=True),\n",
    "    \n",
    "    # resize and crop\n",
    "    Resize(image_resolution),\n",
    "    RandomCrop(image_resolution),\n",
    "    \n",
    "    # normalise pixel values to be between -1 and 1 \n",
    "    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    \n",
    "    # if you're training on greyscale images, convert them to greyscale\n",
    "    # otherwise just do nothing\n",
    "    Grayscale() if colour_channels == 1 else torch.nn.Identity()\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afba6500-322e-4cb9-953a-d04aff923d67",
   "metadata": {},
   "source": [
    "#### **1.2** - Create dataset loaders  \n",
    "\n",
    "Data loaders loads images into batches according to the pre-processes pipeline we have defined previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "580ce8cf-5d73-41b4-bd05-abb6aba20a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "522f0e3e-6f69-4138-9d7f-551daf353c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many images are going to be put into the testing set, \n",
    "# e.g. 0.2 means 20% percent of images\n",
    "test_size = 0.2 \n",
    "\n",
    "# how many images will be used in one epoch, \n",
    "# this usually depend on your model / types of data / CPU or GPU's capability\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47206a61-85a8-4307-87c7-7ffdff64992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instatiate train and test datasets\n",
    "train_dataset = ImageFolder(folder_path, transform=transformation)\n",
    "test_dataset = ImageFolder(folder_path, transform=transformation)\n",
    "\n",
    "# Get length of dataset and indicies\n",
    "num_train = len(train_dataset)\n",
    "indices = list(range(num_train))\n",
    "\n",
    "# Get train / test split for data points\n",
    "train_indices, test_indices = train_test_split(indices, test_size=test_size, random_state=42)\n",
    "\n",
    "# Override dataset classes to only be samples for each split\n",
    "train_sub_dataset = torch.utils.data.Subset(train_dataset, train_indices)\n",
    "test_sub_dataset = torch.utils.data.Subset(test_dataset, test_indices)\n",
    "\n",
    "# Create training and tresing data loaders\n",
    "train_loader = DataLoader(train_sub_dataset, batch_size=batch_size, num_workers=4, multiprocessing_context=\"forkserver\" if device=='mps' else None, shuffle=True)\n",
    "test_loader = DataLoader(test_sub_dataset, batch_size=batch_size, num_workers=4, multiprocessing_context=\"forkserver\" if device=='mps' else None, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fde22f28-1468-45ff-8713-3270f8d01f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196 training images loaded\n",
      "550 testing images loaded\n",
      "classes: ['daisy', 'dandelion', 'rose', 'sunflower', 'tulip']\n",
      "data shape: torch.Size([16, 3, 224, 224])\n",
      "labels shape: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# sort out the names for each classes (according to the folder names)\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "print(f'{len(train_indices)} training images loaded')\n",
    "print(f'{len(test_indices)} testing images loaded')\n",
    "print(f'classes: {class_names}')\n",
    "\n",
    "data, labels = next(iter(train_loader))\n",
    "\n",
    "print(f'data shape: {data.shape}')\n",
    "print(f'labels shape: {labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c874b6-9f50-4110-aada-f147622cff3a",
   "metadata": {},
   "source": [
    "## **Step 2** - Load the pre-trained model  \n",
    "\n",
    "Load the pre-trained VGG11 model from [the torchvision models library](https://pytorch.org/vision/stable/models.html). \n",
    "\n",
    "We are going to do **transfer learning** with this model, which means we are going to start with a trained model rather than a brand new model with random weights.\n",
    "\n",
    "One approach, like we saw in lecture, is to throw away one or more final layers of the original network, and then train new layers to learn the relationship between the activations in the original network and the desired outputs for some new data. (We might also call this \"feature learning\"). \n",
    "\n",
    "Another approach is to take the original network and adjust its weights on some new data. This means that we'll start with the pre-trained model's weight values, rather than start with random weight values, and we will adjust some or all of these values gradually to try to model a new dataset. \n",
    "\n",
    "Both of these transfer learning approaches can work well when you have an existing model that has learned to model something well, and you'd like it to learn to model something slightly different.\n",
    "\n",
    "In this notebook, we'll start by freezing all model parameters (weights) before the last two fully connected layers (i.e. `Linear` layer), and swap the last two fully connected layers with a new `Linear` layer (which will be trained on our new data).\n",
    "\n",
    "We can iterate through all model parameters and set the `requires_grad` attribute to `False`, in this way they are 'freezed', and won't be updated during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "189b7e72-45c3-4ce8-90b3-10e7606b1452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea2b15d-df10-4894-a2c9-09067105178d",
   "metadata": {},
   "source": [
    "**Run this cell if you want to freeze all model parameters in the original network (e.g. because you are replacing the last layers entirely). Or, skip this step if you want to instead fine-tune all weights in the network to reflect the new data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5704445-39d8-4b0a-94a6-b6c7999a8a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all model parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4c87e6-6510-4f76-8a9d-9d9e1da9d604",
   "metadata": {},
   "source": [
    "Replace the last two fully connected layers with new ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5562d361-a508-4d55-8bc1-7ea442ae1058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace the last two fully connected layers with new ones.\n",
    "# model.classifier[3] = nn.Linear(model.classifier[3].in_features, 256)\n",
    "# model.classifier[6] = nn.Linear(256, 5)\n",
    "model.fc = nn.Linear(model.fc.in_features, 5)\n",
    "\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14894534-8b96-4cb7-91e4-15d9048287d4",
   "metadata": {},
   "source": [
    "## **Step 3** - Train the model\n",
    "\n",
    "Now we can used the training workflow to train the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f42c6a-e5d5-40cb-96ee-23ecf5be2f7b",
   "metadata": {},
   "source": [
    "\n",
    "#### Define optimiser and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53e3ad3a-65c8-4e58-9288-a753e332728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy loss\n",
    "loss_function = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd324205-9f5c-4385-a776-1a6addd7692c",
   "metadata": {},
   "source": [
    "#### Define training/ testing loop  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2df78e8-7fbb-4a14-9003-54e55f16afbe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "  -> Step 0001, train loss: 1.7915\n",
      "  -> Step 0051, train loss: 0.9949\n",
      "  -> Step 0101, train loss: 0.7312\n",
      "Epoch 1, train loss: 0.906, test loss: 0.475\n",
      "  -> Step 0001, train loss: 0.7400\n",
      "  -> Step 0051, train loss: 0.6926\n",
      "  -> Step 0101, train loss: 0.5101\n",
      "Epoch 2, train loss: 0.518, test loss: 0.378\n",
      "  -> Step 0001, train loss: 0.5306\n",
      "  -> Step 0051, train loss: 0.6323\n",
      "  -> Step 0101, train loss: 0.7164\n",
      "Epoch 3, train loss: 0.439, test loss: 0.308\n",
      "  -> Step 0001, train loss: 0.5268\n",
      "  -> Step 0051, train loss: 0.3536\n",
      "  -> Step 0101, train loss: 0.4125\n",
      "Epoch 4, train loss: 0.383, test loss: 0.304\n",
      "  -> Step 0001, train loss: 0.3010\n",
      "  -> Step 0051, train loss: 0.3752\n",
      "  -> Step 0101, train loss: 0.2109\n",
      "Epoch 5, train loss: 0.356, test loss: 0.281\n",
      "  -> Step 0001, train loss: 0.0978\n",
      "  -> Step 0051, train loss: 0.3002\n",
      "  -> Step 0101, train loss: 0.2190\n",
      "Epoch 6, train loss: 0.350, test loss: 0.255\n",
      "  -> Step 0001, train loss: 0.5062\n",
      "  -> Step 0051, train loss: 0.2261\n",
      "  -> Step 0101, train loss: 0.0959\n",
      "Epoch 7, train loss: 0.339, test loss: 0.251\n",
      "  -> Step 0001, train loss: 0.2618\n",
      "  -> Step 0051, train loss: 0.3079\n",
      "  -> Step 0101, train loss: 0.2335\n",
      "Epoch 8, train loss: 0.310, test loss: 0.280\n",
      "  -> Step 0001, train loss: 0.2486\n",
      "  -> Step 0051, train loss: 0.1608\n",
      "  -> Step 0101, train loss: 0.1577\n",
      "Epoch 9, train loss: 0.308, test loss: 0.266\n",
      "  -> Step 0001, train loss: 0.2509\n",
      "  -> Step 0051, train loss: 0.3245\n",
      "  -> Step 0101, train loss: 0.0929\n",
      "Epoch 10, train loss: 0.296, test loss: 0.252\n",
      "training finished, model saved to 'model_final.pt'\n"
     ]
    }
   ],
   "source": [
    "# we can save the model regularly\n",
    "save_every_n_epoch = 5\n",
    "\n",
    "# total number of epochs we aim for\n",
    "num_epochs = 10\n",
    "\n",
    "# keep track of the losses, we can plot them in the end\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "print('Epoch 0')\n",
    "\n",
    "for epoch in range(num_epochs): \n",
    "\n",
    "    #---- Training loop -----------------------------\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # Load: The training data loader loads a batch of training data and their true class labels.\n",
    "        inputs, true_labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        true_labels = true_labels.to(device)\n",
    "        \n",
    "        # Pass: Forward pass the training data to our model, and get the predicted classes.\n",
    "        pred_labels = model(inputs)\n",
    "        \n",
    "        # Loss: The loss function compares the predicted classes to the true classes, and calculates the error.\n",
    "        loss = loss_function(pred_labels, true_labels)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Optimise: The optimizer slightly optimises our model based on the error.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(f'  -> Step {i + 1:04}, train loss: {loss.item():.4f}')\n",
    "    \n",
    "    \n",
    "    #---- Testing loop -----------------------------\n",
    "    test_loss = 0.0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        test_loss = 0.0\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            # Load: The testing data loader loads a batch of testing data and their true class labels.\n",
    "            inputs, true_labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            true_labels = true_labels.to(device)\n",
    "            \n",
    "            # Pass: Forward pass the testing data to our model, and get the predicted classes.\n",
    "            pred_labels = model(inputs)\n",
    "            \n",
    "            # Loss: The loss function compares the predicted classes to the true classes, and calculates the error.\n",
    "            loss = loss_function(pred_labels, true_labels)\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    #---- Report some numbers -----------------------------\n",
    "    \n",
    "    # Calculate the cumulative losses in this epoch\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    \n",
    "    # Added cumulative losses to lists for later display\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}, train loss: {train_loss:.3f}, test loss: {test_loss:.3f}')\n",
    "    \n",
    "    # save our model every n epoch\n",
    "    if (epoch+1) % save_every_n_epoch==0:\n",
    "        torch.save(model.state_dict(), f'model_epoch{epoch:04}.pt')\n",
    "        \n",
    "# save the model at the end of the training process\n",
    "torch.save(model.state_dict(), f'model_final.pt')\n",
    "\n",
    "print(\"training finished, model saved to 'model_final.pt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c974e99e-0fd0-486f-9882-8412c2cc0258",
   "metadata": {},
   "source": [
    "#### Plot Training Process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c475d07d-3399-483b-8d9e-c2509ac377a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b44b8efe-9f8b-4521-a69f-7c7773c5cf85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(train_losses, label='training loss')\n",
    "plt.plot(test_losses, label = 'validation loss')\n",
    "plt.xticks(np.arange(len(train_losses)))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d9c6ba-73d4-47e3-b7be-d6338609a5c5",
   "metadata": {},
   "source": [
    "## **Step 4** - Calculate accuracy  \n",
    "\n",
    "Now we can run our model on the full testing set to calculate the accuracy of our model.  \n",
    "\n",
    "Compare the result with the model that we trained from scratch in the first notebook. Does the accuracy improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c3b3f30-2313-49cc-b493-c97af0fee919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct samples: 493  \n",
      "total samples: 550  \n",
      "model accuracy: 0.896\n"
     ]
    }
   ],
   "source": [
    "num_samples = 0\n",
    "num_correct = 0\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        # Load: The testing data loader loads a batch of testing data and their true class labels.\n",
    "        inputs, true_labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        true_labels = true_labels.to(device)\n",
    "\n",
    "        # Pass: Forward pass the testing data to our model, and get the predicted classes.\n",
    "        pred_labels = model(inputs)\n",
    "        pred_labels = torch.argmax(pred_labels, dim=1)\n",
    "        \n",
    "        num_correct += pred_labels.size(0) - torch.count_nonzero(pred_labels - true_labels)\n",
    "        num_samples += pred_labels.size(0) \n",
    "        \n",
    "accuracy = num_correct / num_samples\n",
    "print(f'correct samples: {num_correct}  \\ntotal samples: {num_samples}  \\nmodel accuracy: {accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1951422d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62a0c354",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "After you went through all the code in this notebook, **make a copy of this notebook** before working on the tasks so that we can have a direct comparison of the results.\n",
    "\n",
    "### Task 1\n",
    "Look at other models from the [PyTorch model library](https://pytorch.org/vision/stable/models.html), choose a different pre-trained model (e.g. ResNet18), load it, freeze the weights, and swap the last one or two fully connected layers (just like what we have did for the VGG11 model). Train the model and compare the results (e.g. which model converges faster? Does the accuracy improve?)\n",
    "\n",
    "#### VGG11\n",
    "model accuracy: 0.918\n",
    "\n",
    "#### ResNet18\n",
    "model accuracy: 0.896\n",
    "\n",
    "#### ResNet50\n",
    "crash\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Bonus Task\n",
    "Re-run the code and try out the alternative approach of fine-tuning **all** weights in the network rather than only learning a new final layer (check their differences in Step 2). This means you will skip the step that freezes model parameters for fine-tuning. How do the result and the training process compare? Is this approach converge noticeably slower?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coding3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
