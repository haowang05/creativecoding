{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a89b75e3",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Activate Environment\n",
    "\n",
    "`conda activate myvoiceenv`\n",
    "\n",
    "Install Dependencies\n",
    "\n",
    "`pip install pandas transformers torch`\n",
    "\n",
    "`pip install transformers datasets accelerate`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e8cbc0",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "I downloaded the dataset from Kaggle and used Python to process the data, saving the results as a CSV file.\n",
    "\n",
    "The dataset contains around 40,000 BBC news articles, including information such as title, date, author, link, and description.\n",
    "https://www.kaggle.com/datasets/gpreda/bbc-news/data\n",
    "\n",
    "To classify the emotional tone of the text, I used the following model:\n",
    "https://www.kaggle.com/refs/hf-model/logasanjeev/emotion-analyzer-bert\n",
    "\n",
    "I performed the following processing steps:\n",
    "\n",
    "1. Kept only the news titles.\n",
    "\n",
    "2. Used an AI model to classify the emotions of all 40,000 news titles, and saved the results as a CSV file.\n",
    "\n",
    "3. Mapped the model's 28 emotion labels to 7 facial expression categories.\n",
    "\n",
    "4. Saved the data with news titles and their corresponding emotion labels into a new CSV file.\n",
    "\n",
    "5. Converted the CSV file into TXT format for easier training with GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caf0af0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done! save to news_with_7_emotions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# lodad model\n",
    "model_name = \"logasanjeev/emotions-analyzer-bert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# load CSV\n",
    "df = pd.read_csv(\"../data/bbc_news.csv\")  \n",
    "titles = df[\"title\"].tolist()\n",
    "\n",
    "# original id2label\n",
    "id2label = model.config.id2label\n",
    "\n",
    "# mapping labelsï¼š28 -> 7 \n",
    "mapping = {\n",
    "    'admiration': 'happy',\n",
    "    'amusement': 'happy',\n",
    "    'anger': 'angry',\n",
    "    'annoyance': 'angry',\n",
    "    'approval': 'happy',\n",
    "    'caring': 'happy',\n",
    "    'confusion': 'sad',\n",
    "    'curiosity': 'happy',\n",
    "    'desire': 'happy',\n",
    "    'disappointment': 'sad',\n",
    "    'disapproval': 'angry',\n",
    "    'disgust': 'disgust',\n",
    "    'embarrassment': 'sad',\n",
    "    'excitement': 'happy',\n",
    "    'fear': 'fear',\n",
    "    'gratitude': 'happy',\n",
    "    'grief': 'sad',\n",
    "    'joy': 'happy',\n",
    "    'love': 'happy',\n",
    "    'nervousness': 'fear',\n",
    "    'optimism': 'happy',\n",
    "    'pride': 'happy',\n",
    "    'realization': 'neutral',\n",
    "    'relief': 'happy',\n",
    "    'remorse': 'sad',\n",
    "    'sadness': 'sad',\n",
    "    'surprise': 'surprise',\n",
    "    'neutral': 'neutral'\n",
    "}\n",
    "\n",
    "# dealabel\n",
    "results = []\n",
    "for title in titles:\n",
    "    inputs = tokenizer(title, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = F.softmax(outputs.logits, dim=1)\n",
    "        top_idx = torch.argmax(probs, dim=1).item()\n",
    "        original_label = id2label[top_idx]\n",
    "        mapped_emotion = mapping.get(original_label, \"neutral\")  \n",
    "        results.append(mapped_emotion)\n",
    "\n",
    "# save csv\n",
    "df[\"emotion\"] = results\n",
    "df.to_csv(\"news_with_7_emotions.csv\", index=False)\n",
    "print(\"done! save to news_with_7_emotions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c107a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"news_with_7_emotions.csv\")  \n",
    "with open(\"train.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for _, row in df.iterrows():\n",
    "        f.write(f\"<{row['emotion'].strip()}> {row['title'].strip()}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92af6c67",
   "metadata": {},
   "source": [
    "## Model Testing\n",
    "\n",
    "\n",
    "1. During the initial data processing phase, I split the original dataset into three subsets for feasibility testing: one with around 1,000 entries, another with around 3,000, and the full set with 40,000 entries.\n",
    "\n",
    "2. I tested three different GPT-2 models and saved the results under separate output paths:\n",
    "    1. sshleifer/tiny-gpt2: The generated content was often incoherent and sometimes consisted of non-English words, though it was extremely fast to generate.\n",
    "    2. gpt2ï¼šThe output resembled fake news quite well and remained mostly readable in English. However, the generation speed was slightly slower.\n",
    "    3. gpt2-mediumï¼šThis model pushed my computer to its limits. The generated text was highly readable and felt more realistic, but the generation speed was noticeably slower.\n",
    "3. In the end, I decided to use the gpt2 model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5809db48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc1f6db082ac4db4822e5cdd65e42cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f6dc0fe7e5d4aed81ed32a1b29129bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03232f4c2a5f47c58ed18efb0744a950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8146dfcb87194d1a9e29412d240f1e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8281ae2b110945b7b7d4cfad5ef3373d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d8b7f6bd2244a98a3722491c4a34d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.51M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "/opt/miniconda3/envs/myvoiceenv/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/myvoiceenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a7484c36594684b748aed4043faedf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.51M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='540' max='540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [540/540 00:25, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>10.654300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>10.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>10.626000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>10.617400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>10.612700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/myvoiceenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./gpt2-news-emotion/tokenizer_config.json',\n",
       " './gpt2-news-emotion/special_tokens_map.json',\n",
       " './gpt2-news-emotion/vocab.json',\n",
       " './gpt2-news-emotion/merges.txt',\n",
       " './gpt2-news-emotion/added_tokens.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "# use tiny-gpt2\n",
    "model_name = \"sshleifer/tiny-gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# add emotional tokensï¼ˆ<happy>, <sad> ï¼‰\n",
    "special_tokens = ['<happy>', '<sad>', '<angry>', '<fear>', '<surprise>', '<disgust>', '<neutral>']\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n",
    "model.resize_token_embeddings(len(tokenizer))  \n",
    "\n",
    "# add dataset\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"train.txt\",\n",
    "    block_size=64,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "# training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-news-emotion\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "# start Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer.train()\n",
    "trainer.save_model(\"./gpt2-news-emotion\")\n",
    "tokenizer.save_pretrained(\"./gpt2-news-emotion\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75b3f9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Neil appointed letters weeks Ir Treasury Neil internet Lyn holdersoth Border Bert appointed stop declares my mutual Lebanese\n",
      "ðŸ‘‹\n",
      " internet girl Border save remarkable mutualb firm my Television about deflect Treasury holders makes Lyn weeks Only asked\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model_path = \"./gpt2-news-emotion\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "def generate_title(emotion):\n",
    "    prompt = f\"<{emotion}>\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(input_ids, max_length=20, num_return_sequences=1, do_sample=True, top_k=50, top_p=0.95)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# print generated titles\n",
    "print(generate_title(\"sad\"))\n",
    "print(\"ðŸ‘‹\")\n",
    "print(generate_title(\"happy\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c534a58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dbb33fe876e47f99d4c87e81d7719e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dfced0a612f4a81912af8fa9ad36cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ec846098464f7aa289efe225bded9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aec2ebe2bb74dfea45f472697811eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9186da707d68461798cc8b14fac1e439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52eb3b1583124112a106be8b76ab56f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f18e242b584774bac87b7eacac5271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/myvoiceenv/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/myvoiceenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='540' max='540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [540/540 05:33, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>10.118900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>10.732500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>10.456000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>10.064600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>9.829900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/myvoiceenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./ogpt2-news-emotion/tokenizer_config.json',\n",
       " './ogpt2-news-emotion/special_tokens_map.json',\n",
       " './ogpt2-news-emotion/vocab.json',\n",
       " './ogpt2-news-emotion/merges.txt',\n",
       " './ogpt2-news-emotion/added_tokens.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "# use gpt2\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# add emotion tokensï¼ˆ<happy>, <sad> ï¼‰\n",
    "special_tokens = ['<happy>', '<sad>', '<angry>', '<fear>', '<surprise>', '<disgust>', '<neutral>']\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n",
    "model.resize_token_embeddings(len(tokenizer))  \n",
    "\n",
    "# add dataset\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"train.txt\",\n",
    "    block_size=64,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "# training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ogpt2-news-emotion\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "# start Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer.train()\n",
    "trainer.save_model(\"./ogpt2-news-emotion\")\n",
    "tokenizer.save_pretrained(\"./ogpt2-news-emotion\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f13abba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and refugee cases\n",
      " 'Something is wrong with our system' - what we know from election\n",
      "ðŸ‘‹\n",
      " be suspended and expelled from US cricket team?\n",
      " 'Are there plans for us to rebuild\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model_path = \"./ogpt2-news-emotion\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "def generate_title(emotion):\n",
    "    prompt = f\"<{emotion}>\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(input_ids, max_length=20, num_return_sequences=1, do_sample=True, top_k=50, top_p=0.95,repetition_penalty=1.2, )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generate_title(\"sad\"))\n",
    "print(\"ðŸ‘‹\")\n",
    "print(generate_title(\"happy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f17a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/myvoiceenv/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/myvoiceenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='540' max='540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [540/540 26:00, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.820100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.984600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.040800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.665800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.522100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/myvoiceenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/miniconda3/envs/myvoiceenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/miniconda3/envs/myvoiceenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/miniconda3/envs/myvoiceenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/miniconda3/envs/myvoiceenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/miniconda3/envs/myvoiceenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/miniconda3/envs/myvoiceenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/miniconda3/envs/myvoiceenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/miniconda3/envs/myvoiceenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/miniconda3/envs/myvoiceenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./mgpt2-news-emotion/tokenizer_config.json',\n",
       " './mgpt2-news-emotion/special_tokens_map.json',\n",
       " './mgpt2-news-emotion/vocab.json',\n",
       " './mgpt2-news-emotion/merges.txt',\n",
       " './mgpt2-news-emotion/added_tokens.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "# gpt2-medium\n",
    "model_name = \"gpt2-medium\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "special_tokens = ['<happy>', '<sad>', '<angry>', '<fear>', '<surprise>', '<disgust>', '<neutral>']\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n",
    "model.resize_token_embeddings(len(tokenizer))  \n",
    "\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"train.txt\",\n",
    "    block_size=64,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mgpt2-news-emotion\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./mgpt2-news-emotion\")\n",
    "tokenizer.save_pretrained(\"./mgpt2-news-emotion\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396eb5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Will China step up if Trump takes a step back on climate change?\n",
      " Reeves tells the\n",
      "ðŸ‘‹\n",
      " How will the vulnerable be protected from Covid? And other questions\n",
      " Irish election 'too\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model_path = \"./mgpt2-news-emotion\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "def generate_title(emotion):\n",
    "    prompt = f\"<{emotion}>\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(input_ids, max_length=20, num_return_sequences=1, do_sample=True, top_k=50, top_p=0.95,repetition_penalty=1.2, )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generate_title(\"sad\"))\n",
    "print(\"ðŸ‘‹\")\n",
    "print(generate_title(\"happy\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvoiceenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
