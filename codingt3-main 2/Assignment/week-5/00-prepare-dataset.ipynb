{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week's lab has been adapted from CCI lecturer Terence Broad's [AI for Media materials](https://git.arts.ac.uk/tbroad/AI-4-Media-23-24/). \n",
    "\n",
    "# Prepare dataset\n",
    "\n",
    "In this notebook we are going to prepare our dataset into a binary which is a list of our pre-calculated tokens. (In Natural Language Processing, a \"token\" is the basic unit of text used for analysis or generation; a token could be a single character, a word, or a part of a word. [Read more here](https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/).)\n",
    "\n",
    "Doing this step makes training more efficient as we are not doing that conversion at each training step.\n",
    "\n",
    "First lets do some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from nanoGPT.data import prepare_dataset_as_chars, prepare_dataset_gpt_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure dataset paths\n",
    "\n",
    "By default we are going to load the complete works of shakespeare here, but you can load in whatever text file you want here as your training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = '../media/texts/shakespeare.txt'\n",
    "dataset_path = '../data/class-datasets/text-datasets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset as characters\n",
    "\n",
    "Here we will tokenise the dataset as characters. This is for training simple models like babyGPT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 1,115,394\n",
      "all the unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab size: 65\n",
      "train has 1,003,854 tokens\n",
      "val has 111,540 tokens\n"
     ]
    }
   ],
   "source": [
    "prepare_dataset_as_chars(text_path, os.path.join(dataset_path, 'shakespeare_chars'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset as GPT2 Tokens\n",
    "\n",
    "Here we will tokenise the dataset using OpenAIs tokeniser for GPT2. This is for fine-tuning pre-trained GPT2 models. If you are interested in how they come about these tokenizers, see [Andrej Karpathy's minibpe repo](https://github.com/karpathy/minbpe):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 301,966 tokens\n",
      "val has 36,059 tokens\n"
     ]
    }
   ],
   "source": [
    "prepare_dataset_gpt_tokenizer(text_path, os.path.join(dataset_path, 'shakespeare_tokens'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coding3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
