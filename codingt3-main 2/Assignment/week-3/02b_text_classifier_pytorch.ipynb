{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b3c951b-f0bc-45e8-b669-ba9bba694c96",
   "metadata": {},
   "source": [
    "# Intro to simple text classification in PyTorch\n",
    "\n",
    "This notebook demonstrates training a deep neural network to classify the sentiment (positive or negative) of a movie review.\n",
    "\n",
    "It will take the following setps:\n",
    "\n",
    "**Step 0** - Prepare texts  \n",
    "**Step 1** - Load your dataset  \n",
    "**Step 2** - Create an empty neural network model  \n",
    "**Step 3** - Train the model  \n",
    "**Step 4** - Test and evaluate the training results  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daceeaea-df3f-4110-a09d-b4f222544df9",
   "metadata": {},
   "source": [
    "> Note: If the following codes give you an error like `No module named 'sklearn'` or `No module named 'matplotlib'`, it means you have not yet installed matplotlib or sklearn. To install it, open up a terminal/command window, activate your environment (e.g., `conda activate coding3`), and then type `conda install matplotlib` and `conda install scikit-learn`  \n",
    "You may have to restart the kernel if the cell does not complete running after you install.\n",
    "\n",
    "> Note that you will only ever have to do this once per environment (e.g., from now on, when you run a new notebook that imports from matplotlib using this same environment, it will also be able to use that library; no need to install again)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a4cbae-216a-4208-97f1-539d340dcbcc",
   "metadata": {},
   "source": [
    "Before we start, let's config the device PyTorch is using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a6d10d0-5e44-4124-99d1-817a0de267c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25ffab8-88ba-471b-a369-d45e0379f30b",
   "metadata": {},
   "source": [
    "## **Step 0** - Prepare text  data\n",
    "\n",
    "We're going to use a small set of 1000 movie reviews from IMDB. [The original dataset can be found here.](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?resource=download) \n",
    "\n",
    "Here's how to load in the dataset into the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a1b1fd1-d338-4345-9da8-6f255ac3d4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74dd2151-73fd-4d77-8611-c07b158133d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This assumes the IMDBsubset.csv file lives in a directory called \"data\" which lives in the same directory as this notebook.\n",
    "# ***if you want to edit this notebook to use a different dataset, edit this to specify a different file:\n",
    "df = pd.read_csv(\"data/IMDBsubset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184a16af-1c93-4533-8015-31f2e1f8a62f",
   "metadata": {},
   "source": [
    "Now we've got the data read in! We've used a special data type called a \"data frame\", using the Pandas library, to store this data. Pandas makes working with data pretty convenient.\n",
    "\n",
    "Printing df will show you the data in a table-like format (specifically, it'll show you the first and last few rows of the table).\n",
    "\n",
    "**Note that a sentiment of \"1\" means \"positive\" and \"0\" means \"negative\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea78db99-20bc-4578-a231-854e9b23f7dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.&lt;br /&gt;&lt;br /&gt;The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.&lt;br /&gt;&lt;br /&gt;It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.&lt;br /&gt;&lt;br /&gt;I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. &lt;br /&gt;&lt;br /&gt;The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master's of comedy and his life. &lt;br /&gt;&lt;br /&gt;The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional 'dream' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell's murals decorating every surface) are terribly well done.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love.&lt;br /&gt;&lt;br /&gt;This was the most I'd laughed at one of Woody's comedies in years (dare I say a decade?). While I've never been impressed with Scarlet Johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.&lt;br /&gt;&lt;br /&gt;This may not be the crown jewel of his career, but it was wittier than \"Devil Wears Prada\" and more interesting than \"Superman\" a great comedy to go see with friends.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet &amp; his parents are fighting all the time.&lt;br /&gt;&lt;br /&gt;This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.&lt;br /&gt;&lt;br /&gt;OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing &amp; arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.&lt;br /&gt;&lt;br /&gt;3 out of 10 just for the well playing parents &amp; descent dialogs. As for the shots with Jake: just ignore them.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. &lt;br /&gt;&lt;br /&gt;This being a variation on the Arthur Schnitzler's play about the same theme, the director transfers the action to the present time New York where all these different characters meet and connect. Each one is connected in one way, or another to the next person, but no one seems to know the previous point of contact. Stylishly, the film has a sophisticated luxurious look. We are taken to see how these people live and the world they live in their own habitat.&lt;br /&gt;&lt;br /&gt;The only thing one gets out of all these souls in the picture is the different stages of loneliness each one inhabits. A big city is not exactly the best place in which human relations find sincere fulfillment, as one discerns is the case with most of the people we encounter.&lt;br /&gt;&lt;br /&gt;The acting is good under Mr. Mattei's direction. Steve Buscemi, Rosario Dawson, Carol Kane, Michael Imperioli, Adrian Grenier, and the rest of the talented cast, make these characters come alive.&lt;br /&gt;&lt;br /&gt;We wish Mr. Mattei good luck and await anxiously for his next work.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Nothing is sacred. Just ask Ernie Fosselius. These days, everybody has a video camera, and a movie is hardly out before the spoofs start flying, quickly written and shot, and often posted directly to the internet. Spoofs are hot these days, and we go out of our way to make sure filmmakers don't get off on their own self-importance. 25 years ago, when the first Star Wars was made, it was a different world. Filmmaking was the playground of a select few and spoofs were very rare. Then God gave us Hardware Wars. It was shot to look cheap (or was it just cheap?) and the audio was obviously recorded after the fact. Does that take away from the experience? HECK NO! That's what makes it so great! It was raw and unpolished, and hit relentlessly on some of the more pretentious moments of the original movie. From Fluke Starbucker waving around a flashlight instead of a lightsaber (I did that when I was young!) to Chewchilla the Wookie Monster, to Auggie Ben Doggie's \"nah, just a little headache\" remark, this film short is as much a part of the phenomenon as any of the actual Star Wars films. Rent it. Buy it. Borrow it from a friend. And may the Farce be with you. Always.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>I hated it. I hate self-aware pretentious inanity that masquerades as art. This film is either stupidly inane or inanely stupid. After the first half hour, I fastfowarded through the DVD version, and saw the same juvenile shennanigans over and over and over. I became angered that I had spent hard-earned money for sophomoric clap-trap. Tinting drivel in sepia or blue does not make something a movie, let alone art.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>I usually try to be professional and constructive when I criticize movies, but my GOD!!! This was THE worst movie I have ever seen. Bad acting, bad effects, bad script, bad everything! &lt;br /&gt;&lt;br /&gt;The plot follows a group of teen cliche's on their way to a rave (that takes place in broad daylight) at a remote island. However, when the group arrives, all they find is an empty dance floor and bloody clothes. Determined to find out what happened to the rest of the party-goers, the clan set's off on a mission through a zombie-infested forest. During this crusade, they are aided by a police chick and a sea captain that just happens to have the right number of weapons to give to each of the kids. They also meet up with Jonathan Cherry and some other survivors. Basically the rest of the movie is a collection of poorly directed action sequences including a far too long shootout outside of the \"house of the dead.\" This fight came complete with cheesy Hollywood violence, redundant clips from the HOTD video game, and sloppy matrix-esque camera rotations. One of the character's even volunteers to sacrifice himself to save the others. Why? Not because he was noble and brave, but because part of his face got scarred by acid a zombie spat on him after he continued to beat the creature long after it had been disabled! I'm supposed to feel sorry for this guy?!?&lt;br /&gt;&lt;br /&gt;To sum it all up, there is absolutely no point in seeing this movie unless you want to see for yourself just how terrible it is. The theater I was in was more dead than the zombies on the screen, and I'm sure the money I wasted seeing this piece of sh*t could easily cover the costs it took to make it. GRADE: F</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>If you like me is going to see this in a film history class or something like that at your school, try to convince your teacher to see something else. believe me, anything is better than this movie. it is slow paced, confusing, boring, poorly constructed, gory, gringy, do I need to go on? It's message is good, but I have seen them been handled better in several other films. The acting isn't even any good. This movie is just even more awkward, as it start off as being funny (not intensional though)because of it's surreal story, than at the end, just becomes uncomfortable to watch.&lt;br /&gt;&lt;br /&gt;I honestly feel like 1 hour and 40 minutes of my life has been robbed. Why would anyone want to watch a girls describe a threesome for 10 minutes, than watch them drive through a traffic jam for 20 minutes, listen to a hippie who can make sheep appear, witness a sort of rape, than see the female lead role eat her husband.&lt;br /&gt;&lt;br /&gt;Honestly this movie deserves nothing but a 1/10. And if your not happy with my preview,seriously I'm an open minded guy and I like movies that protest through symbolism, but this movie was just awful. make any excuse you can, to avoid this film.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>This is like a zoology textbook, given that its depiction of animals is so accurate. However, here are a few details that appear to have been slightly modified during the transition to film:&lt;br /&gt;&lt;br /&gt;- Handgun bullets never hit giant Komodo dragons. It doesn't matter how many times you shoot at the Komodo, bullets just won't go near it.&lt;br /&gt;&lt;br /&gt;- The best way to avoid being eaten by a giant Cobra, or a giant Komodo dragon, is just to stand there. The exception to this rule is if you've been told to stay very still, in which case you should run off, until the Komodo is right next to you, and then you should stand there, expecting defeat.&lt;br /&gt;&lt;br /&gt;- Minutes of choppy slow motion footage behind the credits really makes for enjoyable watching.&lt;br /&gt;&lt;br /&gt;- $5,000 is a memory enhancement tool, and an ample substitute for losing your boating license/getting arrested.&lt;br /&gt;&lt;br /&gt;- Members of elite army units don't see giant Komodo dragons coming until they are within one metre of the over-sized beings. Maybe the computer-generated nature of these dragons has something to do with it.&lt;br /&gt;&lt;br /&gt;- When filming a news story aiming on exposing illegal animal testing, a reporter and a cameraman with one camera is all the gear and personnel you will need; sound gear, a second camera, microphones etc are all superfluous.&lt;br /&gt;&lt;br /&gt;- When you hear a loud animal scream, and one person has a gun, he should take it out and point it at the nearest person.&lt;br /&gt;&lt;br /&gt;- When you take a gun out, the sound of the safety being taken off will be made, even if your finger is nowhere near the safety&lt;br /&gt;&lt;br /&gt;- Reporters agree to go half-way around the world in order to expose something - without having the faintest idea what they're exposing. Background research and vague knowledge are out of fashion in modern journalism.&lt;br /&gt;&lt;br /&gt;- Handguns hold at least 52 bullets in one clip, and then more than that in the next clip. Despite that, those with guns claim that they will need more ammo.&lt;br /&gt;&lt;br /&gt;- Expensive cameras (also, remember that the reporter only has one camera) are regularly left behind without even a moment's hesitation or regret. These cameras amazingly manage to make their way back to the reporter all by themselves.&lt;br /&gt;&lt;br /&gt;- The blonde girl really is the stupid one.&lt;br /&gt;&lt;br /&gt;- The same girl that says not to go into a house because a Komodo dragon can easily run right through it, thus making it unsafe, takes a team into a building made of the same material for protection - and nobody says a word about it.&lt;br /&gt;&lt;br /&gt;- High-tech facilities look like simple offices with high school chemistry sets.&lt;br /&gt;&lt;br /&gt;- Genetically-modified snakes grow from normal size to 100 feet long in a matter of a day, but don't grow at all in the weeks either side.&lt;br /&gt;&lt;br /&gt;- The military routinely destroys entire islands when people don't meet contact deadlines.&lt;br /&gt;&lt;br /&gt;- Men with guns don't necessarily change the direction they're shooting when their target is no longer right in front of them. Instead, they just keep shooting into the air.&lt;br /&gt;&lt;br /&gt;- The better looking you are, the greater your chance of surviving giant creatures.&lt;br /&gt;&lt;br /&gt;- Women's intuition is reliable enough to change even the most stubborn of minds.&lt;br /&gt;&lt;br /&gt;- Any time you're being hunted by giant creatures is a great time to hit on girls half your age.&lt;br /&gt;&lt;br /&gt;- Animal noises are an appropriate masking noise for 'swearing' at the same volume.&lt;br /&gt;&lt;br /&gt;- Old Israeli and Russian planes are regularly used by the US Military.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  review  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master's of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional 'dream' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell's murals decorating every surface) are terribly well done.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love.<br /><br />This was the most I'd laughed at one of Woody's comedies in years (dare I say a decade?). While I've never been impressed with Scarlet Johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.<br /><br />This may not be the crown jewel of his career, but it was wittier than \"Devil Wears Prada\" and more interesting than \"Superman\" a great comedy to go see with friends.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. <br /><br />This being a variation on the Arthur Schnitzler's play about the same theme, the director transfers the action to the present time New York where all these different characters meet and connect. Each one is connected in one way, or another to the next person, but no one seems to know the previous point of contact. Stylishly, the film has a sophisticated luxurious look. We are taken to see how these people live and the world they live in their own habitat.<br /><br />The only thing one gets out of all these souls in the picture is the different stages of loneliness each one inhabits. A big city is not exactly the best place in which human relations find sincere fulfillment, as one discerns is the case with most of the people we encounter.<br /><br />The acting is good under Mr. Mattei's direction. Steve Buscemi, Rosario Dawson, Carol Kane, Michael Imperioli, Adrian Grenier, and the rest of the talented cast, make these characters come alive.<br /><br />We wish Mr. Mattei good luck and await anxiously for his next work.   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ...   \n",
       "995                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Nothing is sacred. Just ask Ernie Fosselius. These days, everybody has a video camera, and a movie is hardly out before the spoofs start flying, quickly written and shot, and often posted directly to the internet. Spoofs are hot these days, and we go out of our way to make sure filmmakers don't get off on their own self-importance. 25 years ago, when the first Star Wars was made, it was a different world. Filmmaking was the playground of a select few and spoofs were very rare. Then God gave us Hardware Wars. It was shot to look cheap (or was it just cheap?) and the audio was obviously recorded after the fact. Does that take away from the experience? HECK NO! That's what makes it so great! It was raw and unpolished, and hit relentlessly on some of the more pretentious moments of the original movie. From Fluke Starbucker waving around a flashlight instead of a lightsaber (I did that when I was young!) to Chewchilla the Wookie Monster, to Auggie Ben Doggie's \"nah, just a little headache\" remark, this film short is as much a part of the phenomenon as any of the actual Star Wars films. Rent it. Buy it. Borrow it from a friend. And may the Farce be with you. Always.   \n",
       "996                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     I hated it. I hate self-aware pretentious inanity that masquerades as art. This film is either stupidly inane or inanely stupid. After the first half hour, I fastfowarded through the DVD version, and saw the same juvenile shennanigans over and over and over. I became angered that I had spent hard-earned money for sophomoric clap-trap. Tinting drivel in sepia or blue does not make something a movie, let alone art.   \n",
       "997                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            I usually try to be professional and constructive when I criticize movies, but my GOD!!! This was THE worst movie I have ever seen. Bad acting, bad effects, bad script, bad everything! <br /><br />The plot follows a group of teen cliche's on their way to a rave (that takes place in broad daylight) at a remote island. However, when the group arrives, all they find is an empty dance floor and bloody clothes. Determined to find out what happened to the rest of the party-goers, the clan set's off on a mission through a zombie-infested forest. During this crusade, they are aided by a police chick and a sea captain that just happens to have the right number of weapons to give to each of the kids. They also meet up with Jonathan Cherry and some other survivors. Basically the rest of the movie is a collection of poorly directed action sequences including a far too long shootout outside of the \"house of the dead.\" This fight came complete with cheesy Hollywood violence, redundant clips from the HOTD video game, and sloppy matrix-esque camera rotations. One of the character's even volunteers to sacrifice himself to save the others. Why? Not because he was noble and brave, but because part of his face got scarred by acid a zombie spat on him after he continued to beat the creature long after it had been disabled! I'm supposed to feel sorry for this guy?!?<br /><br />To sum it all up, there is absolutely no point in seeing this movie unless you want to see for yourself just how terrible it is. The theater I was in was more dead than the zombies on the screen, and I'm sure the money I wasted seeing this piece of sh*t could easily cover the costs it took to make it. GRADE: F   \n",
       "998                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           If you like me is going to see this in a film history class or something like that at your school, try to convince your teacher to see something else. believe me, anything is better than this movie. it is slow paced, confusing, boring, poorly constructed, gory, gringy, do I need to go on? It's message is good, but I have seen them been handled better in several other films. The acting isn't even any good. This movie is just even more awkward, as it start off as being funny (not intensional though)because of it's surreal story, than at the end, just becomes uncomfortable to watch.<br /><br />I honestly feel like 1 hour and 40 minutes of my life has been robbed. Why would anyone want to watch a girls describe a threesome for 10 minutes, than watch them drive through a traffic jam for 20 minutes, listen to a hippie who can make sheep appear, witness a sort of rape, than see the female lead role eat her husband.<br /><br />Honestly this movie deserves nothing but a 1/10. And if your not happy with my preview,seriously I'm an open minded guy and I like movies that protest through symbolism, but this movie was just awful. make any excuse you can, to avoid this film.   \n",
       "999  This is like a zoology textbook, given that its depiction of animals is so accurate. However, here are a few details that appear to have been slightly modified during the transition to film:<br /><br />- Handgun bullets never hit giant Komodo dragons. It doesn't matter how many times you shoot at the Komodo, bullets just won't go near it.<br /><br />- The best way to avoid being eaten by a giant Cobra, or a giant Komodo dragon, is just to stand there. The exception to this rule is if you've been told to stay very still, in which case you should run off, until the Komodo is right next to you, and then you should stand there, expecting defeat.<br /><br />- Minutes of choppy slow motion footage behind the credits really makes for enjoyable watching.<br /><br />- $5,000 is a memory enhancement tool, and an ample substitute for losing your boating license/getting arrested.<br /><br />- Members of elite army units don't see giant Komodo dragons coming until they are within one metre of the over-sized beings. Maybe the computer-generated nature of these dragons has something to do with it.<br /><br />- When filming a news story aiming on exposing illegal animal testing, a reporter and a cameraman with one camera is all the gear and personnel you will need; sound gear, a second camera, microphones etc are all superfluous.<br /><br />- When you hear a loud animal scream, and one person has a gun, he should take it out and point it at the nearest person.<br /><br />- When you take a gun out, the sound of the safety being taken off will be made, even if your finger is nowhere near the safety<br /><br />- Reporters agree to go half-way around the world in order to expose something - without having the faintest idea what they're exposing. Background research and vague knowledge are out of fashion in modern journalism.<br /><br />- Handguns hold at least 52 bullets in one clip, and then more than that in the next clip. Despite that, those with guns claim that they will need more ammo.<br /><br />- Expensive cameras (also, remember that the reporter only has one camera) are regularly left behind without even a moment's hesitation or regret. These cameras amazingly manage to make their way back to the reporter all by themselves.<br /><br />- The blonde girl really is the stupid one.<br /><br />- The same girl that says not to go into a house because a Komodo dragon can easily run right through it, thus making it unsafe, takes a team into a building made of the same material for protection - and nobody says a word about it.<br /><br />- High-tech facilities look like simple offices with high school chemistry sets.<br /><br />- Genetically-modified snakes grow from normal size to 100 feet long in a matter of a day, but don't grow at all in the weeks either side.<br /><br />- The military routinely destroys entire islands when people don't meet contact deadlines.<br /><br />- Men with guns don't necessarily change the direction they're shooting when their target is no longer right in front of them. Instead, they just keep shooting into the air.<br /><br />- The better looking you are, the greater your chance of surviving giant creatures.<br /><br />- Women's intuition is reliable enough to change even the most stubborn of minds.<br /><br />- Any time you're being hunted by giant creatures is a great time to hit on girls half your age.<br /><br />- Animal noises are an appropriate masking noise for 'swearing' at the same volume.<br /><br />- Old Israeli and Russian planes are regularly used by the US Military.   \n",
       "\n",
       "     sentiment  \n",
       "0            1  \n",
       "1            1  \n",
       "2            1  \n",
       "3            0  \n",
       "4            1  \n",
       "..         ...  \n",
       "995          1  \n",
       "996          0  \n",
       "997          0  \n",
       "998          0  \n",
       "999          0  \n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None) \n",
    "# display.max_colwidth means: show me everything in the column, even if it's long!\n",
    "\n",
    "df # it will display the first and last few examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35cc3d5-2730-43be-85cb-070993e3ca5a",
   "metadata": {},
   "source": [
    "## **Step 1** - Load your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27272b6e-b978-4878-88ec-5be747f5af68",
   "metadata": {},
   "source": [
    "#### **1.1** - Pre-process texts\n",
    "\n",
    "Let's do something super simple to transform this into a dataset that we can send to a neural network. \n",
    "\n",
    "Similarly to the sentiment classification we discussed in lecture last week, we're going to represent each example (review) as a vector of word counts.\n",
    "\n",
    "The CountVectorizer object from the 'sklearn' library allows us to make these word count vectors pretty easily. Once we do the counts, we'll store these in a new dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51f2e16-3bff-4097-bda4-c1e8653aa71f",
   "metadata": {},
   "source": [
    "Transform a dataframe containing a column named `review`,  \n",
    "such that each row becomes represented by a **set of word counts**, corresponding to the number of each term in the review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e35c0124-ee5d-4255-88cf-c2fc34db6405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06905760-c70b-49d8-b97e-70ae267e4b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following codes perform word counting:\n",
    "\n",
    "# remove numeric characters present in the text\n",
    "def preprocess_text(text):\n",
    "    return re.sub(r'\\d+', '', text.lower())\n",
    "\n",
    "# stop_words='english' removes very common english words that are unlikely to be useful (e.g. \"and\", \"the\")\n",
    "# min_df=0.1 removes very rare words that are likely to be typos, uninformative, etc.\n",
    "vectorizer = CountVectorizer(stop_words='english', preprocessor=preprocess_text, min_df=0.01)\n",
    "\n",
    "#***Note that \"df['review']\" is used below because \"review\" is the name of the column containing our text in the dataframe\n",
    "#If you apply this to your own data, you may probably need to change this column name!\n",
    "matrix = vectorizer.fit_transform(df['review'])\n",
    "    \n",
    "#This line converts matrix into another dataframe, with column names corresponding to the word being counted\n",
    "data = pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897c4b24-0d0b-4932-97d9-945419181915",
   "metadata": {},
   "source": [
    "Take a look at the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9d7d3e8-da9c-4028-a2e4-cabb141a23af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ability  able  absolutely  absurd  academy  accent  accents  accept  \\\n",
      "0          0     0           0       0        0       0        0       0   \n",
      "1          0     0           0       0        0       0        0       0   \n",
      "2          0     0           0       0        0       0        0       0   \n",
      "3          0     0           0       0        0       0        0       0   \n",
      "4          0     0           0       0        0       0        0       0   \n",
      "..       ...   ...         ...     ...      ...     ...      ...     ...   \n",
      "995        0     0           0       0        0       0        0       0   \n",
      "996        0     0           0       0        0       0        0       0   \n",
      "997        0     0           1       0        0       0        0       0   \n",
      "998        0     0           0       0        0       0        0       0   \n",
      "999        0     0           0       0        0       0        0       0   \n",
      "\n",
      "     accident  account  ...  year  years  yes  york  young  younger  youth  \\\n",
      "0           0        0  ...     0      0    0     0      0        0      0   \n",
      "1           0        0  ...     0      0    0     0      0        0      0   \n",
      "2           0        0  ...     0      1    0     0      1        0      0   \n",
      "3           0        0  ...     0      0    0     0      0        0      0   \n",
      "4           0        0  ...     0      0    0     1      0        0      0   \n",
      "..        ...      ...  ...   ...    ...  ...   ...    ...      ...    ...   \n",
      "995         0        0  ...     0      1    0     0      1        0      0   \n",
      "996         0        0  ...     0      0    0     0      0        0      0   \n",
      "997         0        0  ...     0      0    0     0      0        0      0   \n",
      "998         0        0  ...     0      0    0     0      0        0      0   \n",
      "999         0        0  ...     0      0    0     0      0        0      0   \n",
      "\n",
      "     zero  zombie  zombies  \n",
      "0       0       0        0  \n",
      "1       0       0        0  \n",
      "2       0       0        0  \n",
      "3       0       2        0  \n",
      "4       0       0        0  \n",
      "..    ...     ...      ...  \n",
      "995     0       0        0  \n",
      "996     0       0        0  \n",
      "997     0       2        1  \n",
      "998     0       0        0  \n",
      "999     0       0        0  \n",
      "\n",
      "[1000 rows x 1660 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 1660)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_wordcount = data.shape[1]\n",
    "print(data) #prints data to screen\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dad0a56-424d-4932-88cb-4234d98fbea4",
   "metadata": {},
   "source": [
    "We have 1000 rows and 1660 columns, this means that we have 1000 reviews, and there are 1660 unique words in all these reviews, the numbers in the table "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92bacef-0455-4cc9-ad19-4c64cc5884ae",
   "metadata": {},
   "source": [
    "Note that you can examine this dataset, e.g. to look at the column of counts for the word \"wonderful\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "117cb136-7154-4042-9ba4-5c1bb2ccea01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      2\n",
       "      ..\n",
       "995    0\n",
       "996    0\n",
       "997    0\n",
       "998    2\n",
       "999    0\n",
       "Name: good, Length: 1000, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"good\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439d31f7-632f-4c54-bf08-3340496e1e2b",
   "metadata": {},
   "source": [
    "My output says:\n",
    "```\n",
    "0      0\n",
    "1      0\n",
    "2      0\n",
    "3      0\n",
    "4      2\n",
    "      ..\n",
    "995    0\n",
    "996    0\n",
    "997    0\n",
    "998    2\n",
    "999    0\n",
    "```\n",
    "\n",
    "This means that the word \"good\" appears in the 4th review twice, appears in the 998th review twice, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4346c3b-972e-4d69-8679-9226f138e669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "995    0\n",
       "996    1\n",
       "997    0\n",
       "998    0\n",
       "999    0\n",
       "Name: hate, Length: 1000, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or the word \"hate\":\n",
    "data[\"hate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d184d4de-61ed-43f2-84c1-a451846a1939",
   "metadata": {},
   "source": [
    "Let's create a mapping function that turn a line of text into wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c08b343-3b6d-499f-8339-92bf3758d954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turns a text string into a dataframe example (***Note you'll need to change this from 'review' for your own dataset)\n",
    "def createExample(myText):\n",
    "\n",
    "    newExample = np.array([[myText]])\n",
    "    tdf = pd.DataFrame(newExample, columns=[\"review\"])\n",
    "    matrix = vectorizer.transform(tdf['review'])\n",
    "    newDf = pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    return newDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db2937cb-fda6-44f5-a298-58a6fc05adef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ability  able  absolutely  absurd  academy  accent  accents  accept  \\\n",
      "0        0     0           1       0        0       0        0       0   \n",
      "\n",
      "   accident  account  ...  year  years  yes  york  young  younger  youth  \\\n",
      "0         0        0  ...     0      0    0     0      0        0      0   \n",
      "\n",
      "   zero  zombie  zombies  \n",
      "0     0       0        3  \n",
      "\n",
      "[1 rows x 1660 columns]\n"
     ]
    }
   ],
   "source": [
    "#Here's a text about zombies\n",
    "myText = \"This movie is absolutely zombies zombies zombies\"\n",
    "t = createExample(myText)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175591a7-0411-4d0a-a8e9-f680b4632a4c",
   "metadata": {},
   "source": [
    "When we print the dataframe above, you see wordcount for \"absolutely\" is 1, wordcount for \"zombies\" is 3.\n",
    "\n",
    "You might have noticed that the count for most words will be 0 for any particular text. Sometimes it's useful to look at just the nonzero word counts. Here's a quick function to do that for the output of our createExample function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81ab48eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "absolutely    1\n",
       "movie         1\n",
       "zombies       3\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getNonzeroCounts(myExample):\n",
    "    counts = myExample.iloc[0]\n",
    "    return(counts[counts != 0])\n",
    "\n",
    "getNonzeroCounts(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1a2cb8-8cfc-4cdb-9477-2501179c1ecd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **1.2** - Create dataset loaders  \n",
    "\n",
    "In order to load images during training, we're going to use something called \"data loader\".  \n",
    "These data loaders loads images into batches according to the pre-processes pipeline we have defined previously.\n",
    "\n",
    "We'll be using PyTorch's [Datasets & DataLoaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) to create a training data loader and a testing data loader.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f873402-ac77-4acb-b502-7bfc64d40feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "849a8470-ed0d-4da1-a680-c3f3c9533b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many texts are going to be put into the testing set, \n",
    "# e.g. 0.1 means 10% percent of texts\n",
    "test_size = 0.1 \n",
    "\n",
    "# how many texts will be used in one step of gradient descent, \n",
    "# this usually depend on your model / types of data / CPU or GPU's capability\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e11d66-73d2-4843-8928-baec6958fcc2",
   "metadata": {},
   "source": [
    "There is a lot of discussion about how different batch sizes can affect training results, e.g. [here](https://www.cs.princeton.edu/~smalladi/blog/2024/01/22/SDEs-ScalingRules/). But as a rule of thumb: larger batch sizes can make your gradient less noisy and therefore safer for larger learning rates, but this is always subjected to different models and data, especially when training generative models. So a good way to decide batch sizes is to follow literatures or similar works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "779b27a7-2201-4707-b762-2c73ceda5830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember: X is for input, y is for output\n",
    "# The first argument of train_test_split is your training data (here, lives in \"data\" object you created using word counts)\n",
    "# The second argument of train_test_split is your labels/targets for the training data. This lives in the \"sentiment\" column of the original dataframe df we loaded from the file.\n",
    "# (***If you are using a different dataset, you'll need to change the name of this column to whatever it is in your dataset)\n",
    "# The test_size argument specifies % of data going into test set: here, 20% of the data goes into test set and 80% goes into training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, df['sentiment'], test_size=test_size, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a44aa55-4456-4870-a9d0-e0986e6e472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your datset\n",
    "train_dataset = TensorDataset(torch.Tensor(X_train.values), torch.Tensor(y_train.values))\n",
    "# create your dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=2, shuffle=True) \n",
    "\n",
    "# create your datset\n",
    "test_dataset = TensorDataset(torch.Tensor(X_test.values), torch.Tensor(y_test.values)) \n",
    "# create your dataloader\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=2, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8d28b5b-40c4-4a97-aa71-275bd5d959fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 training texts loaded\n",
      "100 testing texts loaded\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(train_dataset)} training texts loaded')\n",
    "print(f'{len(test_dataset)} testing texts loaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2c0015-3013-43b7-9049-b3180e6c11ff",
   "metadata": {},
   "source": [
    "#### **1.3** - Check our data\n",
    "Now we have the `train_loader` and the `test_loader` objects,  \n",
    "We can use them to load and visualise some of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbf196f8-05b3-4b46-9507-d786f45a2240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: torch.Size([16, 1660])\n",
      "labels shape: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "data, labels = next(iter(train_loader))\n",
    "\n",
    "print(f'data shape: {data.shape}')\n",
    "print(f'labels shape: {labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf42de3-2664-4770-877f-a48929ba15ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Step 2** - Load an empty neural network model   \n",
    "\n",
    "We're going to skip over the details of how to setup a neural network model for now, they will be covered next week. If you want to have a sneak peek, feel free to check the `./src/model.py` file.  \n",
    "\n",
    "For now, trust that the code makes a new neural network according to your dataset configurations.  \n",
    "However, there're three important parameters:\n",
    "\n",
    " - `num_features` defines the number of unique word we have.\n",
    " - `num_classes` defines how many categories we are classifying  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36bd06e8-c958-4d84-857f-3263cbe75f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import DenseNeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "865f8ca2-fd35-43bc-bdbf-cfbb5bd920b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseNeuralNetwork(\n",
       "  (dense): ModuleList(\n",
       "    (0): Linear(in_features=1660, out_features=32, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DenseNeuralNetwork(num_features = num_wordcount)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e2138f-2b77-4241-8f09-759c9105cbad",
   "metadata": {},
   "source": [
    "## **Step 3** - Train the model\n",
    "\n",
    "#### **3.1** - Define optimiser and loss function\n",
    "\n",
    "First, let's define an optimizer and the type of error (loss) function:\n",
    " - we use the `adam` optimizer\n",
    " - we use the binary cross entropy `BCELoss` as our loss function \n",
    " \n",
    "This means: during the training process, the adam optimizer will try to minimise errors calculated by the binary cross entropy loss ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bdcabcd-b30f-43d4-9173-6ab0f9500c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92343788-71ff-471a-9234-75b1d3b2ebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary cross entropy\n",
    "loss_function = torch.nn.BCELoss()\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91088e74-ae7c-49c4-baad-8764087c966a",
   "metadata": {},
   "source": [
    "#### **3.2** - Break and Recap\n",
    "\n",
    "Up to this point, we have defined a lot of components for the model training.   \n",
    "Let's recap what we have so far:\n",
    " - A training data loader `train_loader` and a testing data loader `test_loader` we defined in **Step 1**.\n",
    " - An empty neural network `model` we built in **Step 2**.\n",
    " - An optimizer `optimizer` and a loss function `loss_function` we just defined.\n",
    "\n",
    "In the next step, we're going to program how they will coordinate during training. So..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff80f0b-0ca9-41f0-b742-abfcc88ee134",
   "metadata": {},
   "source": [
    "#### **3.3** - Define training/ testing loop  \n",
    "\n",
    "As an overview, each epoch has a training loop and a testing loop:   \n",
    "In each training loop:\n",
    " - **Load**: The training data loader loads a batch of training data and their true class labels.\n",
    " - **Forward**: Forward pass the training data to our model, and get the predicted class labels.\n",
    " - **Loss**: Use the loss function to compares the predicted labels to the true labels, and compute the error.\n",
    " - **Optimise**: The optimizer slightly optimises our model based on the error computed by the loss function.\n",
    "\n",
    "Once we finish a training loop, we do a testing loop. In a testing loop, we do exactly the same things as in a training loop, but we won't update our model. The testing results are only used to tell us how well the model is doing at the moment.\n",
    "\n",
    "In each testing loop:\n",
    " - **Load**: The testing data loader loads a batch of testing data and their true class labels.\n",
    " - **Forward**: Forward pass the testing data to our model, and get the predicted class labels.\n",
    " - **Loss**: Use the loss function to compares the predicted labels to the true labels, and calculates the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30376c53-0f64-4365-be2d-0dc793eb8417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "  -> Step 0001, train loss: 0.7118\n",
      "  -> Step 0051, train loss: 0.6784\n",
      "Epoch 1, train loss: 0.675, test loss: 0.649\n",
      "  -> Step 0001, train loss: 0.6443\n",
      "  -> Step 0051, train loss: 0.5844\n",
      "Epoch 2, train loss: 0.602, test loss: 0.581\n",
      "  -> Step 0001, train loss: 0.5518\n",
      "  -> Step 0051, train loss: 0.4586\n",
      "Epoch 3, train loss: 0.520, test loss: 0.540\n",
      "  -> Step 0001, train loss: 0.4810\n",
      "  -> Step 0051, train loss: 0.4921\n",
      "Epoch 4, train loss: 0.434, test loss: 0.457\n",
      "  -> Step 0001, train loss: 0.3336\n",
      "  -> Step 0051, train loss: 0.3878\n",
      "Epoch 5, train loss: 0.356, test loss: 0.428\n",
      "  -> Step 0001, train loss: 0.2986\n",
      "  -> Step 0051, train loss: 0.2238\n",
      "Epoch 6, train loss: 0.289, test loss: 0.405\n",
      "  -> Step 0001, train loss: 0.2066\n",
      "  -> Step 0051, train loss: 0.2648\n",
      "Epoch 7, train loss: 0.233, test loss: 0.360\n",
      "  -> Step 0001, train loss: 0.1985\n",
      "  -> Step 0051, train loss: 0.1978\n",
      "Epoch 8, train loss: 0.187, test loss: 0.367\n",
      "  -> Step 0001, train loss: 0.2328\n",
      "  -> Step 0051, train loss: 0.1191\n",
      "Epoch 9, train loss: 0.159, test loss: 0.349\n",
      "  -> Step 0001, train loss: 0.1475\n",
      "  -> Step 0051, train loss: 0.2228\n",
      "Epoch 10, train loss: 0.130, test loss: 0.370\n",
      "training finished, model saved to 'text_model_final.pt'\n"
     ]
    }
   ],
   "source": [
    "# we can save the model regularly\n",
    "save_every_n_epoch = 10\n",
    "\n",
    "# total number of epochs we aim for\n",
    "num_epochs = 10\n",
    "\n",
    "# keep track of the losses, we can plot them in the end\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "print('Epoch 0')\n",
    "\n",
    "for epoch in range(num_epochs): \n",
    "    \n",
    "    \n",
    "    #---- Training loop -----------------------------\n",
    "    train_loss = 0.0\n",
    "    model.train() \n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # Load: The training data loader loads a batch of training data and their true class labels.\n",
    "        inputs, true_labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        true_labels = true_labels.to(device)\n",
    "        \n",
    "        # Pass: Forward pass the training data to our model, and get the predicted classes.\n",
    "        pred_labels = model(inputs)\n",
    "        \n",
    "        # Loss: The loss function compares the predicted classes to the true classes, and calculates the error.\n",
    "        loss = loss_function(pred_labels, true_labels.unsqueeze(1))\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Optimise: The optimizer slightly optimises our model based on the error.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(f'  -> Step {i + 1:04}, train loss: {loss.item():.4f}')\n",
    "    \n",
    "    \n",
    "    #---- Testing loop -----------------------------\n",
    "    test_loss = 0.0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            # Load: The testing data loader loads a batch of testing data and their true class labels.\n",
    "            inputs, true_labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            true_labels = true_labels.to(device)\n",
    "            \n",
    "            # Pass: Forward pass the testing data to our model, and get the predicted classes.\n",
    "            pred_labels = model(inputs)\n",
    "            \n",
    "            # Loss: The loss function compares the predicted classes to the true classes, and calculates the error.\n",
    "            loss = loss_function(pred_labels, true_labels.unsqueeze(1))\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    #---- Report some numbers -----------------------------\n",
    "    \n",
    "    # Calculate the cumulative losses in this epoch\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    \n",
    "    # Added cumulative losses to lists for later display\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}, train loss: {train_loss:.3f}, test loss: {test_loss:.3f}')\n",
    "    \n",
    "    # save our model every n epoch\n",
    "    if (epoch+1) % save_every_n_epoch==0:\n",
    "        torch.save(model.state_dict(), f'text_model_epoch{epoch:04}.pt')\n",
    "        \n",
    "# save the model at the end of the training process\n",
    "torch.save(model.state_dict(), f'text_model_final.pt')\n",
    "\n",
    "print(\"training finished, model saved to 'text_model_final.pt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0538b6fa-9480-479f-b8a1-6c298fb40e3a",
   "metadata": {},
   "source": [
    "#### **3.4** - Plot Training Process\n",
    "\n",
    "Plot how errors (\"loss\") change over time on the training set and the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa1f27de-bbf1-42df-bfe4-cec2a2d2b5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a099e609-ebda-4831-96e0-4902847cabd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(train_losses, label='training loss')\n",
    "plt.plot(test_losses, label = 'validation loss')\n",
    "plt.xticks(np.arange(len(train_losses)))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec3a849-b302-4ccc-9bdd-471cb2f07ac6",
   "metadata": {},
   "source": [
    "## **Step 4** - Test and evaluate the training results   \n",
    "\n",
    "#### **4.1** - Load model  \n",
    "First, let's load our trained model. And set the model to 'evaluation' mode `.eval()` (check [torch.nn.Module.eval()](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10f4613c-d0b4-450e-b578-d3de48c52f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import DenseNeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2cfc618-fb16-4acb-882b-c9f42bc91fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseNeuralNetwork(\n",
       "  (dense): ModuleList(\n",
       "    (0): Linear(in_features=1660, out_features=32, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure the parameters are the same as when the model is created\n",
    "eval_model = DenseNeuralNetwork(num_features = num_wordcount)\n",
    "\n",
    "# load the saved model, make sure the path is correct\n",
    "eval_model.load_state_dict(torch.load('text_model_final.pt'))\n",
    "\n",
    "eval_model.to(device)\n",
    "eval_model.eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c1f602-b66d-4bd6-9d61-f37eeffc93ba",
   "metadata": {},
   "source": [
    "#### **4.2** - Load some test data \n",
    "\n",
    "Data in the testing set has never been exposed to the model during the training process, so they are good to test if our model works fine on \"new\" data. Why? Because sometimes the model begins to **overfit** to the training set and is not able to \"generalise\" to data that are not in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3984b912-77a5-4a18-80a7-3fb9421d089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this into something you made up:\n",
    "test_text = 'A wonderful little production. The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, sense of realism to the entire piece.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d1c2a7c-636d-4e8e-acea-1448e8818b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"negative\", \"positive\"]\n",
    "test_data = createExample(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f71f587-9938-45e1-8be0-6251727ef6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    pred_labels = eval_model(torch.Tensor(test_data.values).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39317d0-2e79-46f9-8874-1499024a356a",
   "metadata": {},
   "source": [
    "#### **4.3** - Interpret the output\n",
    "\n",
    "`pred_labels` is our model's output. It's not a single class label, instead, it's a number indicating the probability of `label == 1`, i.e., the likelihood of the text is positive.\n",
    "\n",
    "We can use `torch.round()` to round the output to 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e1bec8b-63f2-4ed5-b53b-0fb500e52958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: A wonderful little production. The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, sense of realism to the entire piece.\n",
      "\n",
      "probability: 0.6358627676963806\n",
      "prediction: positive\n"
     ]
    }
   ],
   "source": [
    "print(f'text: {test_text}\\n')\n",
    "print(f'probability: {pred_labels.item()}')\n",
    "\n",
    "pred_labels = class_names[int(torch.round(pred_labels)[0][0])]\n",
    "print(f'prediction: {pred_labels}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee5aae4-7976-4079-9463-4c3e7d553fe8",
   "metadata": {},
   "source": [
    "#### **4.4** - Calculate accuracy  \n",
    "\n",
    "Now we can run our model on the full testing set to calculate the accuracy of our model.  \n",
    "In practice, we'll keep counting the correct predictions and check what percentage of the predictions are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6a071e5-e687-49a7-8f33-803ec39deefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct samples: 84  \n",
      "total samples: 100  \n",
      "model accuracy: 0.840\n"
     ]
    }
   ],
   "source": [
    "num_samples = 0\n",
    "num_correct = 0\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        # Load: The testing data loader loads a batch of testing data and their true class labels.\n",
    "        inputs, true_labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        true_labels = true_labels.to(device)\n",
    "\n",
    "        # Pass: Forward pass the testing data to our model, and get the predicted classes.\n",
    "        pred_labels = eval_model(inputs)[:,0]\n",
    "        pred_labels = torch.round(pred_labels)\n",
    "        \n",
    "        num_correct += pred_labels.size(0) - torch.count_nonzero(pred_labels - true_labels)\n",
    "        num_samples += pred_labels.size(0) \n",
    "        \n",
    "accuracy = num_correct / num_samples\n",
    "print(f'correct samples: {num_correct}  \\ntotal samples: {num_samples}  \\nmodel accuracy: {accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de59b2f-d845-4cc5-943f-38c7d6d68185",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coding3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
